{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install datasets llama-index==0.10.37 llama-index-embeddings-openai==0.1.9 qdrant-client==1.9.1 llama-index-vector-stores-qdrant==0.2.8 llama-index-llms-cohere==0.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install datasets llama-index-embeddings-fastembed llama-index-llms-mistralai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: you should install the following packages to your environment:\n",
    "\n",
    "`pip install datasets`\n",
    "\n",
    "`pip install llama-index-embeddings-fastembed`\n",
    "\n",
    "`pip install llama-index-llms-mistralai`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     10\u001b[39m load_dotenv()\n\u001b[32m     12\u001b[39m sys.path.append(\u001b[33m'\u001b[39m\u001b[33m../helpers\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m setup_llm, setup_embed_model, create_index\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hands-on-ai-rag-using-llamaindex-3830207/03_Introduction_to_RAG/../helpers/utils.py:3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrandom\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtime\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m defaultdict\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from getpass import getpass\n",
    "import nest_asyncio\n",
    "from IPython.display import Markdown, display\n",
    "from datasets import Dataset\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "sys.path.append('../helpers')\n",
    "\n",
    "from utils import setup_llm, setup_embed_model, create_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CO_API_KEY = os.environ['CO_API_KEY'] or getpass(\"Enter your Cohere API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY'] or getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QDRANT_URL = os.environ['QDRANT_URL'] or getpass(\"Enter your Qdrant URL:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QDRANT_API_KEY = os.environ['QDRANT_API_KEY'] or  getpass(\"Enter your Qdrant API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.settings import Settings\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "from utils import setup_llm, setup_embed_model, setup_vector_store, create_index\n",
    "\n",
    "setup_llm(provider=\"ollama\", model=\"\", api_key=\"\")\n",
    "\n",
    "setup_embed_model(provider=\"cohere\", api_key=CO_API_KEY)\n",
    "\n",
    "COLLECTION_NAME = \"it_can_be_done\"\n",
    "\n",
    "vector_store = setup_vector_store(QDRANT_URL, QDRANT_API_KEY, COLLECTION_NAME)\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "index = create_index(from_where=\"vector_store\", vector_store=vector_store, storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import display_prompt_dict, create_query_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = create_query_engine(\n",
    "    index=index,\n",
    "    similarity_top_k=3, \n",
    "    mode=\"query\",\n",
    "    return_sources=True\n",
    "    )\n",
    "\n",
    "display_prompt_dict(query_engine.get_prompts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.prompts import PromptTemplate\n",
    "\n",
    "custom_prompt = \"\"\"You are an assistant for question-answering tasks related to \\\n",
    "motivational poetry. Your must reponse with an original Haiku style poem.\n",
    "\n",
    "Use the following pieces of retrieved context to answer the user's query:\n",
    "\n",
    "---------------------\\n\n",
    "{context_str}\\n\n",
    "---------------------\\n\n",
    "\n",
    "Query: {query_str}\n",
    "\"\"\"\n",
    "\n",
    "custom_prompt_template = PromptTemplate(custom_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine.update_prompts(\n",
    "    {\"response_synthesizer:text_qa_template\": custom_prompt_template}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_prompt_dict(query_engine.get_prompts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import Workflow, step, StartEvent, StopEvent\n",
    "\n",
    "class QueryEvent(StartEvent):\n",
    "    input: str\n",
    "\n",
    "class ResponseEvent(StopEvent):\n",
    "    result: str\n",
    "\n",
    "class QueryWorkflow(Workflow):\n",
    "    @step\n",
    "    async def run_query(self, ev: QueryEvent) -> ResponseEvent:\n",
    "        response = query_engine.query(ev.input)\n",
    "        return ResponseEvent(result=str(response))\n",
    "\n",
    "workflow = QueryWorkflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "async def run_workflow():\n",
    "    response = await workflow.run(input=\"If you keep your head when all around you are losing their cool and blaming it on you\")\n",
    "    return response\n",
    "\n",
    "result = asyncio.run(run_workflow())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Response Synthesizers\n",
    "\n",
    "The Llama Index [documentation](https://docs.llamaindex.ai/en/stable/module_guides/querying/response_synthesizers/) has a lot of detail regarding each of the response sythensizers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.response_synthesizers import ResponseMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(ResponseMode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Response modes\n",
    "\n",
    "In LlamaIndex, [response modes](https://docs.llamaindex.ai/en/stable/module_guides/deploying/query_engine/response_modes/) are used to determine how the system should process and return the results of a query.  Each response mode is designed to handle different types of queries and use cases, providing flexibility and customization in how you interact with your data.\n",
    "\n",
    "\n",
    "\n",
    "#### âš—ï¸ Refine \n",
    "\n",
    "Refine is an iterative method to generate a response. \n",
    "\n",
    "Initially, we use the context in the first node and the query to create a basic answer. Then, we refine this answer by inputting it, along with the query and context of the second node, into a \"refine prompt\" to generate an improved answer. \n",
    "\n",
    "This refinement process continues through N-1 nodes, with N being the total number of nodes. It makes a separate LLM call per Node/retrieved chunk. This mode is good for generating more detailed answers.\n",
    "\n",
    "#### ðŸ¤ Compact\n",
    "\n",
    "Compact and refine mode first combine text chunks into larger consolidated chunks that more fully utilize the available context window, then refine answers across them. This mode is faster than refine since we make fewer calls to the LLM. \n",
    "\n",
    "This mode is useful when you want to reduce the number of LLM calls while still refining the answer.\n",
    "\n",
    "\n",
    "#### ðŸ“ Simple summarize\n",
    "\n",
    "Merge all text chunks into one and make a large language model call. The call will fail if the merged text chunk exceeds the context window size.\n",
    "\n",
    "It's good for quick summarization purposes, but may lose detail due to truncation.\n",
    "\n",
    "#### ðŸŒ´ Tree summarize\n",
    "\n",
    "Construct a tree index for the candidate nodes in a bottom-up manner then use a summary prompt based on the query. Return the root node as the final response. When this mode is set, the system is instructed to iterate through many, if not all, documents in order to synthesize an answer, which can lead to better summarization results. \n",
    "\n",
    "This mode is particularly useful for summarization queries, where the goal is to provide a comprehensive summary of a collection of text or a specific topic.\n",
    "\n",
    "#### ðŸ¤– Generation\n",
    "\n",
    "Ignore context, just use LLM to generate a response.\n",
    "\n",
    "#### âŒ No text\n",
    "\n",
    "This mode only runs the retriever to fetch the nodes that would have been sent to the LLM, without actually synthesizing a final response. The nodes can then be inspected by checking `response.source_nodes`.\n",
    "\n",
    "#### ðŸ“ Accumulate\n",
    "\n",
    "This mode applies the query to each text chunk while accumulating the responses into an array. It returns a concatenated string of all responses. \n",
    "\n",
    "This mode is good for when you need to run the same query separately against each text chunk.\n",
    "\n",
    "#### Compact accumulate\n",
    "\n",
    "In the compact and accumulate mode, text chunks are combined into larger chunks to utilize the context window better. Answers are then accumulated for each chunk and returned as a concatenation. This mode is faster than accumulate as it reduces calls to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.workflow import Workflow, step, StartEvent, StopEvent\n",
    "\n",
    "response_synthesizer = get_response_synthesizer(response_mode=\"compact\")\n",
    "\n",
    "query_engine = create_query_engine(\n",
    "    index,\n",
    "    mode=\"query\",\n",
    "    response_synthesizer = response_synthesizer\n",
    "    )\n",
    "\n",
    "class QueryEvent(StartEvent):\n",
    "    input: str\n",
    "\n",
    "class ResponseEvent(StopEvent):\n",
    "    result: str\n",
    "\n",
    "class SynthWorkflow(Workflow):\n",
    "    @step\n",
    "    async def synth(self, ev: QueryEvent) -> ResponseEvent:\n",
    "        response = query_engine.query(ev.input)\n",
    "        return ResponseEvent(result=str(response))\n",
    "\n",
    "workflow = SynthWorkflow()\n",
    "\n",
    "import asyncio\n",
    "\n",
    "async def run_workflow():\n",
    "    response = await workflow.run(input=\"What do the poems teach about one should think about success and failure?\")\n",
    "    return response\n",
    "\n",
    "result = asyncio.run(run_workflow())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lil_llama_index",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
