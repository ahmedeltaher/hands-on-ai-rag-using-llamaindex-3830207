{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install llama-index llama-index-embeddings-cohere qdrant-client llama-index-vector-stores-qdrant llama-index-llms-cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install llama-index-llms-ollama llama-index-embeddings-cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from getpass import getpass\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CO_API_KEY = os.environ['CO_API_KEY'] or getpass(\"Enter your Cohere API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove OpenAI API key since we're not using OpenAI\n",
    "# OPENAI_API_KEY = os.environ['OPENAI_API_KEY'] or getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QDRANT_URL = os.environ['QDRANT_URL'] or getpass(\"Enter your Qdrant URL:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QDRANT_API_KEY = os.environ['QDRANT_API_KEY'] or  getpass(\"Enter your Qdrant API Key:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Workflows\n",
    "\n",
    "<img src=\"https://docs.llamaindex.ai/en/stable/_static/query/pipeline_rag_example.png\">\n",
    "\n",
    "Source: [LlamaIndex Docs](https://docs.llamaindex.ai/en/stable/module_guides/querying/pipeline/)\n",
    "\n",
    "LlamaIndex offers a workflow API for chaining modules to manage data workflows easily. It revolves around the Workflow, where you link various modules like LLMs, prompts, and retrievers in a sequence or DAG for end-to-end execution using events.\n",
    "\n",
    "You can streamline workflows efficiently using Workflow, reducing code complexity and enhancing readability. Additionally, a declarative interface ensures easy serialization of workflow components for portability and deployment across systems in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.settings import Settings\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.cohere import CohereEmbedding\n",
    "\n",
    "Settings.llm = Ollama(\n",
    "    model=\"qwen2.5:7b\",\n",
    "    request_timeout=120.0,\n",
    "    context_window=8000,\n",
    ")\n",
    "\n",
    "Settings.embed_model = CohereEmbedding(\n",
    "    api_key=CO_API_KEY,\n",
    "    model_name=\"embed-english-v3.0\"\n",
    ")\n",
    "\n",
    "# Debug: Verify embedding model is set\n",
    "print(f\"Embedding model type: {type(Settings.embed_model)}\")\n",
    "print(f\"Embedding model: {Settings.embed_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import AsyncQdrantClient\n",
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "\n",
    "# Create a Qdrant client\n",
    "client = AsyncQdrantClient(\n",
    "    url=QDRANT_URL, \n",
    "    api_key=QDRANT_API_KEY,\n",
    ")\n",
    "\n",
    "# Create a Qdrant vector store\n",
    "vector_store = QdrantVectorStore(\n",
    "    aclient=client, \n",
    "    collection_name=\"it_can_be_done\"\n",
    "    )\n",
    "\n",
    "# Create a vector store index with explicit embed_model\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store=vector_store,\n",
    "    embed_model=Settings.embed_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A RAG Workflow with PromptTemplate\n",
    "\n",
    "I'm going to kick it off with a slightly complex workflow where the input is passes through two prompts before initiating retrieval.\n",
    "\n",
    "1. Retrieve question about given topic.\n",
    "\n",
    "2. Rephrase the context\n",
    "\n",
    "Each prompt only takes in one input, so `Workflow` will automatically chain LLM outputs into the prompt and then into the LLM using events.\n",
    "\n",
    "You'll see how to define event flows more explicitly in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.core.workflow import (\n",
    "    Event,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Workflow,\n",
    "    step,\n",
    ")\n",
    "\n",
    "# Define events for the workflow\n",
    "class TopicEvent(Event):\n",
    "    topic: str\n",
    "\n",
    "class ContextEvent(Event):\n",
    "    context: str\n",
    "\n",
    "class ResponseEvent(Event):\n",
    "    response: str\n",
    "\n",
    "# generate question regarding topic\n",
    "prompt_str1 = \"Retrieve context about the following topic: {topic}\"\n",
    "prompt_tmpl1 = PromptTemplate(prompt_str1)\n",
    "\n",
    "prompt_str2 = \"\"\"Synthesize the context provided into an answer using modern slang, while still quoting the sources.\n",
    "\n",
    "Context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Synthesized response:\n",
    "\"\"\"\n",
    "\n",
    "prompt_tmpl2 = PromptTemplate(prompt_str2)\n",
    "\n",
    "retriever = index.as_retriever(similarity_top_k=5)\n",
    "\n",
    "class RAGWorkflow(Workflow):\n",
    "    @step\n",
    "    async def generate_query(self, ev: StartEvent) -> TopicEvent:\n",
    "        topic = ev.topic\n",
    "        query = prompt_tmpl1.format(topic=topic)\n",
    "        return TopicEvent(topic=query)\n",
    "    \n",
    "    @step\n",
    "    async def retrieve_context(self, ev: TopicEvent) -> ContextEvent:\n",
    "        nodes = await retriever.aretrieve(ev.topic)\n",
    "        context = \"\\n\\n\".join([node.text for node in nodes])\n",
    "        return ContextEvent(context=context)\n",
    "    \n",
    "    @step\n",
    "    async def synthesize_response(self, ev: ContextEvent) -> StopEvent:\n",
    "        prompt = prompt_tmpl2.format(context=ev.context)\n",
    "        response = await Settings.llm.acomplete(prompt)\n",
    "        return StopEvent(result=str(response))\n",
    "\n",
    "w = RAGWorkflow(timeout=60, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "async def run_workflow():\n",
    "    response = await w.run(topic=\"Working hard to achieve your goals even when you doubt yourself and your chances of success\")\n",
    "    return response\n",
    "\n",
    "response = asyncio.run(run_workflow())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can debug the workflow by viewing the event history and intermediate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_workflow_with_debug():\n",
    "    handler = w.run(topic=\"Working hard to achieve your goals even when you doubt yourself and your chances of success\")\n",
    "    async for event in handler.stream_events():\n",
    "        print(f\"Event: {event}\")\n",
    "    result = await handler\n",
    "    return result\n",
    "\n",
    "result = asyncio.run(run_workflow_with_debug())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another RAG Workflow\n",
    "\n",
    "Here we setup a RAG workflow without the query rewriting step.\n",
    "\n",
    "Here we need a way to link the input query to both the retriever and summarizer. \n",
    "\n",
    "We can do this by defining events that can be consumed by multiple steps, allowing us to link the inputs to multiple downstream modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.response_synthesizers import TreeSummarize\n",
    "\n",
    "retriever = index.as_retriever(similarity_top_k=5)\n",
    "tree_summarizer = TreeSummarize(llm=Settings.llm)\n",
    "\n",
    "class QueryEvent(Event):\n",
    "    query: str\n",
    "\n",
    "class NodesEvent(Event):\n",
    "    nodes: list\n",
    "\n",
    "class SimpleRAGWorkflow(Workflow):\n",
    "    @step\n",
    "    async def retrieve_nodes(self, ev: StartEvent) -> NodesEvent:\n",
    "        nodes = await retriever.aretrieve(ev.query)\n",
    "        return NodesEvent(nodes=nodes)\n",
    "    \n",
    "    @step\n",
    "    async def synthesize_response(self, ev: NodesEvent) -> StopEvent:\n",
    "        # Get the original query from the context\n",
    "        query = self._get_original_query()\n",
    "        response = await tree_summarizer.asynthesize(query, ev.nodes)\n",
    "        return StopEvent(result=response)\n",
    "    \n",
    "    def _get_original_query(self):\n",
    "        # Access the original query from workflow context if needed\n",
    "        return getattr(self, '_original_query', '')\n",
    "\n",
    "w2 = SimpleRAGWorkflow(timeout=60, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The workflow is already configured with the steps and event flows\n",
    "# No need for manual module addition and linking like in QueryPipeline\n",
    "print(\"Workflow configured with automatic event routing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_simple_workflow():\n",
    "    # Store original query in workflow for access in steps\n",
    "    w2._original_query = \"Working hard to achieve your goals even when you doubt yourself and your chances of success\"\n",
    "    response = await w2.run(query=\"Working hard to achieve your goals even when you doubt yourself and your chances of success\")\n",
    "    return response\n",
    "\n",
    "response = asyncio.run(run_simple_workflow())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access response attributes\n",
    "if hasattr(response, '__dict__'):\n",
    "    print(response.__dict__)\n",
    "else:\n",
    "    print(f\"Response type: {type(response)}\")\n",
    "    print(f\"Response: {response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lil_llama_index",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
