{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install datasets llama-index==0.10.37 llama-index-embeddings-cohere qdrant-client==1.9.1 llama-index-vector-stores-qdrant==0.2.8 llama-index-llms-cohere==0.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip3 install datasets llama-index-embeddings-fastembed llama-index-llms-mistralai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install llama-index-llms-ollama llama-index-embeddings-cohere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: you should install the following packages to your environment:\n",
    "\n",
    "`pip install datasets`\n",
    "\n",
    "`pip install llama-index-embeddings-fastembed`\n",
    "\n",
    "`pip install llama-index-llms-mistralai`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from datasets import Dataset\n",
    "from dotenv import load_dotenv\n",
    "from getpass import getpass\n",
    "import nest_asyncio\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "sys.path.append('../helpers')\n",
    "\n",
    "from utils import setup_llm, setup_embed_model, create_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CO_API_KEY = os.environ['CO_API_KEY'] or getpass(\"Enter your Cohere API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove OpenAI API key since we're not using OpenAI\n",
    "# OPENAI_API_KEY = os.environ['OPENAI_API_KEY'] or getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "QDRANT_URL = os.environ['QDRANT_URL'] or getpass(\"Enter your Qdrant URL:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "QDRANT_API_KEY = os.environ['QDRANT_API_KEY'] or  getpass(\"Enter your Qdrant API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model type: <class 'llama_index.embeddings.cohere.base.CohereEmbedding'>\n",
      "Embedding model: model_name='embed-english-v3.0' embed_batch_size=10 callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x30cae9590> num_workers=None embeddings_cache=None api_key='D837umjWnWmrAcfabry3it8bn3ZZs74TOLi73Zqp' base_url=None truncate='END' input_type=None embedding_type='float'\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.settings import Settings\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "from utils import setup_llm, setup_embed_model, setup_vector_store, create_index\n",
    "\n",
    "setup_llm(provider=\"ollama\", model=\"\", api_key=\"\")\n",
    "setup_embed_model(provider=\"cohere\", api_key=CO_API_KEY)\n",
    "\n",
    "# Debug: Check if embedding model is set correctly\n",
    "print(f\"Embedding model type: {type(Settings.embed_model)}\")\n",
    "print(f\"Embedding model: {Settings.embed_model}\")\n",
    "\n",
    "COLLECTION_NAME = \"it_can_be_done\"\n",
    "\n",
    "vector_store = setup_vector_store(QDRANT_URL, QDRANT_API_KEY, COLLECTION_NAME)\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "# Explicitly pass the embed_model to ensure it's used\n",
    "index = create_index(\n",
    "    from_where=\"vector_store\", \n",
    "    vector_store=vector_store, \n",
    "    storage_context=storage_context,\n",
    "    embed_model=Settings.embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import display_prompt_dict, create_query_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " **Prompt Key**: response_synthesizer:text_qa_template\n",
       "**Text:**\n",
       "```\n",
       "Context information is below.\n",
       "---------------------\n",
       "{context_str}\n",
       "---------------------\n",
       "Given the context information and not prior knowledge, answer the query.\n",
       "Query: {query_str}\n",
       "Answer: \n",
       "```\n",
       "\n",
       "**Prompt Key**: response_synthesizer:refine_template\n",
       "**Text:**\n",
       "```\n",
       "The original query is as follows: {query_str}\n",
       "We have provided an existing answer: {existing_answer}\n",
       "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
       "------------\n",
       "{context_msg}\n",
       "------------\n",
       "Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\n",
       "Refined Answer: \n",
       "```\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_engine = create_query_engine(\n",
    "    index=index,\n",
    "    similarity_top_k=3, \n",
    "    mode=\"query\",\n",
    "    return_sources=True\n",
    "    )\n",
    "\n",
    "display_prompt_dict(query_engine.get_prompts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.prompts import PromptTemplate\n",
    "\n",
    "custom_prompt = \"\"\"You are an assistant for question-answering tasks related to \\\n",
    "motivational poetry. Your must reponse with an original Haiku style poem.\n",
    "\n",
    "Use the following pieces of retrieved context to answer the user's query:\n",
    "\n",
    "---------------------\\n\n",
    "{context_str}\\n\n",
    "---------------------\\n\n",
    "\n",
    "Query: {query_str}\n",
    "\"\"\"\n",
    "\n",
    "custom_prompt_template = PromptTemplate(custom_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine.update_prompts(\n",
    "    {\"response_synthesizer:text_qa_template\": custom_prompt_template}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " **Prompt Key**: response_synthesizer:text_qa_template\n",
       "**Text:**\n",
       "```\n",
       "You are an assistant for question-answering tasks related to motivational poetry. Your must reponse with an original Haiku style poem.\n",
       "\n",
       "Use the following pieces of retrieved context to answer the user's query:\n",
       "\n",
       "---------------------\n",
       "\n",
       "{context_str}\n",
       "\n",
       "---------------------\n",
       "\n",
       "\n",
       "Query: {query_str}\n",
       "\n",
       "```\n",
       "\n",
       "**Prompt Key**: response_synthesizer:refine_template\n",
       "**Text:**\n",
       "```\n",
       "The original query is as follows: {query_str}\n",
       "We have provided an existing answer: {existing_answer}\n",
       "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
       "------------\n",
       "{context_msg}\n",
       "------------\n",
       "Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\n",
       "Refined Answer: \n",
       "```\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_prompt_dict(query_engine.get_prompts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5p/trf91_p17bg3d0t9l5pl9kdm0000gn/T/ipykernel_83703/4238623573.py:6: UserWarning: Field name \"result\" in \"ResponseEvent\" shadows an attribute in parent \"StopEvent\"\n",
      "  class ResponseEvent(StopEvent):\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.workflow import Workflow, step, StartEvent, StopEvent\n",
    "\n",
    "class QueryEvent(StartEvent):\n",
    "    input: str\n",
    "\n",
    "class ResponseEvent(StopEvent):\n",
    "    result: str\n",
    "\n",
    "class QueryWorkflow(Workflow):\n",
    "    @step\n",
    "    async def run_query(self, ev: QueryEvent) -> ResponseEvent:\n",
    "        response = query_engine.query(ev.input)\n",
    "        return ResponseEvent(result=str(response))\n",
    "\n",
    "workflow = QueryWorkflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "async def run_workflow():\n",
    "    response = await workflow.run(input=\"If you keep your head when all around you are losing their cool and blaming it on you\")\n",
    "    return response\n",
    "\n",
    "result = asyncio.run(run_workflow())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head held high amidst storm,\n",
      "Blame dances like leaves in wind.\n",
      "Stay true, hold on tight.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Response Synthesizers\n",
    "\n",
    "The Llama Index [documentation](https://docs.llamaindex.ai/en/stable/module_guides/querying/response_synthesizers/) has a lot of detail regarding each of the response sythensizers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.response_synthesizers import ResponseMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ACCUMULATE',\n",
       " 'COMPACT',\n",
       " 'COMPACT_ACCUMULATE',\n",
       " 'CONTEXT_ONLY',\n",
       " 'GENERATION',\n",
       " 'NO_TEXT',\n",
       " 'REFINE',\n",
       " 'SIMPLE_SUMMARIZE',\n",
       " 'TREE_SUMMARIZE',\n",
       " '__add__',\n",
       " '__class__',\n",
       " '__contains__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__getnewargs__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__members__',\n",
       " '__mod__',\n",
       " '__module__',\n",
       " '__mul__',\n",
       " '__name__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__qualname__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__rmod__',\n",
       " '__rmul__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " 'capitalize',\n",
       " 'casefold',\n",
       " 'center',\n",
       " 'count',\n",
       " 'encode',\n",
       " 'endswith',\n",
       " 'expandtabs',\n",
       " 'find',\n",
       " 'format',\n",
       " 'format_map',\n",
       " 'index',\n",
       " 'isalnum',\n",
       " 'isalpha',\n",
       " 'isascii',\n",
       " 'isdecimal',\n",
       " 'isdigit',\n",
       " 'isidentifier',\n",
       " 'islower',\n",
       " 'isnumeric',\n",
       " 'isprintable',\n",
       " 'isspace',\n",
       " 'istitle',\n",
       " 'isupper',\n",
       " 'join',\n",
       " 'ljust',\n",
       " 'lower',\n",
       " 'lstrip',\n",
       " 'maketrans',\n",
       " 'partition',\n",
       " 'removeprefix',\n",
       " 'removesuffix',\n",
       " 'replace',\n",
       " 'rfind',\n",
       " 'rindex',\n",
       " 'rjust',\n",
       " 'rpartition',\n",
       " 'rsplit',\n",
       " 'rstrip',\n",
       " 'split',\n",
       " 'splitlines',\n",
       " 'startswith',\n",
       " 'strip',\n",
       " 'swapcase',\n",
       " 'title',\n",
       " 'translate',\n",
       " 'upper',\n",
       " 'zfill']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(ResponseMode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Response modes\n",
    "\n",
    "In LlamaIndex, [response modes](https://docs.llamaindex.ai/en/stable/module_guides/deploying/query_engine/response_modes/) are used to determine how the system should process and return the results of a query.  Each response mode is designed to handle different types of queries and use cases, providing flexibility and customization in how you interact with your data.\n",
    "\n",
    "\n",
    "\n",
    "#### âš—ï¸ Refine \n",
    "\n",
    "Refine is an iterative method to generate a response. \n",
    "\n",
    "Initially, we use the context in the first node and the query to create a basic answer. Then, we refine this answer by inputting it, along with the query and context of the second node, into a \"refine prompt\" to generate an improved answer. \n",
    "\n",
    "This refinement process continues through N-1 nodes, with N being the total number of nodes. It makes a separate LLM call per Node/retrieved chunk. This mode is good for generating more detailed answers.\n",
    "\n",
    "#### ðŸ¤ Compact\n",
    "\n",
    "Compact and refine mode first combine text chunks into larger consolidated chunks that more fully utilize the available context window, then refine answers across them. This mode is faster than refine since we make fewer calls to the LLM. \n",
    "\n",
    "This mode is useful when you want to reduce the number of LLM calls while still refining the answer.\n",
    "\n",
    "\n",
    "#### ðŸ“ Simple summarize\n",
    "\n",
    "Merge all text chunks into one and make a large language model call. The call will fail if the merged text chunk exceeds the context window size.\n",
    "\n",
    "It's good for quick summarization purposes, but may lose detail due to truncation.\n",
    "\n",
    "#### ðŸŒ´ Tree summarize\n",
    "\n",
    "Construct a tree index for the candidate nodes in a bottom-up manner then use a summary prompt based on the query. Return the root node as the final response. When this mode is set, the system is instructed to iterate through many, if not all, documents in order to synthesize an answer, which can lead to better summarization results. \n",
    "\n",
    "This mode is particularly useful for summarization queries, where the goal is to provide a comprehensive summary of a collection of text or a specific topic.\n",
    "\n",
    "#### ðŸ¤– Generation\n",
    "\n",
    "Ignore context, just use LLM to generate a response.\n",
    "\n",
    "#### âŒ No text\n",
    "\n",
    "This mode only runs the retriever to fetch the nodes that would have been sent to the LLM, without actually synthesizing a final response. The nodes can then be inspected by checking `response.source_nodes`.\n",
    "\n",
    "#### ðŸ“ Accumulate\n",
    "\n",
    "This mode applies the query to each text chunk while accumulating the responses into an array. It returns a concatenated string of all responses. \n",
    "\n",
    "This mode is good for when you need to run the same query separately against each text chunk.\n",
    "\n",
    "#### Compact accumulate\n",
    "\n",
    "In the compact and accumulate mode, text chunks are combined into larger chunks to utilize the context window better. Answers are then accumulated for each chunk and returned as a concatenation. This mode is faster than accumulate as it reduces calls to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5p/trf91_p17bg3d0t9l5pl9kdm0000gn/T/ipykernel_83703/881716511.py:15: UserWarning: Field name \"result\" in \"ResponseEvent\" shadows an attribute in parent \"StopEvent\"\n",
      "  class ResponseEvent(StopEvent):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The poems suggest that success lies more in how we use the resources at our disposal rather than solely depending on external factors such as money or social prestige. They emphasize that each individual has sufficient materials for achievement, and what truly matters is the spirit or willpower one brings to the task.\n",
      "\n",
      "For instance, \"OPPORTUNITY\" highlights that regardless of past failures or current setbacks, every morning offers new opportunities to succeed, with the poem's speaker rising to start anew daily. Similarly, in \"WORK,\" the idea that work itself is a powerful force for personal development and success is underscored, encouraging individuals to embrace their potential by actively engaging in labor.\n",
      "\n",
      "Overall, these poems teach that one should focus on proactive effort and persistent application rather than dwelling on past mistakes or waiting for external conditions to change.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.workflow import Workflow, step, StartEvent, StopEvent\n",
    "\n",
    "response_synthesizer = get_response_synthesizer(response_mode=\"compact\")\n",
    "\n",
    "query_engine = create_query_engine(\n",
    "    index,\n",
    "    mode=\"query\",\n",
    "    response_synthesizer = response_synthesizer\n",
    "    )\n",
    "\n",
    "class QueryEvent(StartEvent):\n",
    "    input: str\n",
    "\n",
    "class ResponseEvent(StopEvent):\n",
    "    result: str\n",
    "\n",
    "class SynthWorkflow(Workflow):\n",
    "    @step\n",
    "    async def synth(self, ev: QueryEvent) -> ResponseEvent:\n",
    "        response = query_engine.query(ev.input)\n",
    "        return ResponseEvent(result=str(response))\n",
    "\n",
    "workflow = SynthWorkflow()\n",
    "\n",
    "import asyncio\n",
    "\n",
    "async def run_workflow():\n",
    "    response = await workflow.run(input=\"What do the poems teach about one should think about success and failure?\")\n",
    "    return response\n",
    "\n",
    "result = asyncio.run(run_workflow())\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lil_llama_index",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
