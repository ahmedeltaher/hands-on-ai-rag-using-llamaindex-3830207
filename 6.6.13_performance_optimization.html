<!DOCTYPE html>
<html lang="ar" dir="rtl">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Document Summary Index - Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø«Ø§Ù„Ø« Ø¹Ø´Ø±: ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡ ÙˆØ§Ù„Ø³Ø±Ø¹Ø©</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.8;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            padding: 20px;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: rgba(255, 255, 255, 0.95);
            border-radius: 20px;
            box-shadow: 0 20px 40px rgba(0,0,0,0.1);
            overflow: hidden;
        }

        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 2rem;
            text-align: center;
        }

        .header h1 {
            font-size: 2.5em;
            margin-bottom: 0.5rem;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.3);
        }

        .header p {
            font-size: 1.2em;
            opacity: 0.9;
        }

        .content {
            padding: 2rem;
        }

        .card {
            background: white;
            margin: 1.5rem 0;
            border-radius: 15px;
            box-shadow: 0 8px 25px rgba(0,0,0,0.1);
            overflow: hidden;
            transition: transform 0.3s ease, box-shadow 0.3s ease;
        }

        .card:hover {
            transform: translateY(-5px);
            box-shadow: 0 15px 35px rgba(0,0,0,0.15);
        }

        .card h2 {
            background: linear-gradient(135deg, #2d3436 0%, #636e72 100%);
            color: white;
            padding: 1.5rem;
            margin: 0;
            font-size: 1.8em;
            border-top: 5px solid #667eea;
        }

        .card-content {
            padding: 1.5rem;
        }

        .code-block {
            background: #2d3436;
            color: #ddd;
            padding: 1.5rem;
            border-radius: 10px;
            margin: 1rem 0;
            overflow-x: auto;
            direction: ltr;
            text-align: left;
            font-family: 'Courier New', monospace;
            border-left: 5px solid #00b894;
        }

        .highlight-box {
            background: linear-gradient(135deg, #00b894 0%, #00cec9 100%);
            color: white;
            padding: 1.5rem;
            border-radius: 10px;
            margin: 1rem 0;
            border-left: 5px solid #00a085;
        }

        .warning-box {
            background: linear-gradient(135deg, #e17055 0%, #fab1a0 100%);
            color: white;
            padding: 1.5rem;
            border-radius: 10px;
            margin: 1rem 0;
            border-left: 5px solid #d63031;
        }

        .info-box {
            background: linear-gradient(135deg, #74b9ff 0%, #0984e3 100%);
            color: white;
            padding: 1.5rem;
            border-radius: 10px;
            margin: 1rem 0;
            border-left: 5px solid #2d3436;
        }

        .performance-box {
            background: linear-gradient(135deg, #a29bfe 0%, #6c5ce7 100%);
            color: white;
            padding: 1.5rem;
            border-radius: 10px;
            margin: 1rem 0;
            border-left: 5px solid #5f3dc4;
        }

        @media (max-width: 768px) {
            .container {
                margin: 10px;
                border-radius: 10px;
            }
            
            .header h1 {
                font-size: 1.8em;
            }
            
            .content {
                padding: 1rem;
            }

            .code-block {
                font-size: 0.8em;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>âš¡ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡ ÙˆØ§Ù„Ø³Ø±Ø¹Ø©</h1>
            <p>Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø«Ø§Ù„Ø« Ø¹Ø´Ø±: ØªÙ‚Ù†ÙŠØ§Øª Ù…ØªÙ‚Ø¯Ù…Ø© Ù„ØªØ³Ø±ÙŠØ¹ Document Summary Index</p>
        </div>

        <div class="content">
            <div class="card">
                <h2>ğŸ¯ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡</h2>
                <div class="card-content">
                    <p>Ù‡Ù†Ø§Ùƒ Ø¹Ø¯Ø© Ù…Ø³ØªÙˆÙŠØ§Øª Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡ ÙÙŠ Document Summary Index:</p>

                    <div class="performance-box">
                        <h4>ğŸš€ Ø§Ù„Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„Ø£Ø±Ø¨Ø¹Ø© Ù„Ù„ØªØ­Ø³ÙŠÙ†</h4>
                        <ol>
                            <li><strong>ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:</strong> Ø¥Ø¹Ø¯Ø§Ø¯ ÙˆØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø­ØªÙˆÙ‰</li>
                            <li><strong>ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†Ù…Ø§Ø°Ø¬:</strong> Ø§Ø®ØªÙŠØ§Ø± ÙˆØ§Ø³ØªØ®Ø¯Ø§Ù… Ø£ÙØ¶Ù„</li>
                            <li><strong>ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©:</strong> ØªØ³Ø±ÙŠØ¹ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª</li>
                            <li><strong>ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªØ®Ø²ÙŠÙ†:</strong> Ø­ÙØ¸ ÙˆØ§Ø³ØªØ±Ø¬Ø§Ø¹ Ø£Ø³Ø±Ø¹</li>
                        </ol>
                    </div>

                    <div class="code-block">
# ØªØ­Ù„ÙŠÙ„ Ù†Ù‚Ø§Ø· Ø§Ù„Ø¶Ø¹Ù ÙÙŠ Ø§Ù„Ø£Ø¯Ø§Ø¡
import time
from typing import List, Dict
import asyncio

class PerformanceProfiler:
    def __init__(self):
        self.bottlenecks = {}
        
    def identify_bottlenecks(self, documents: List):
        """ØªØ­Ø¯ÙŠØ¯ Ù†Ù‚Ø§Ø· Ø§Ù„Ø¶Ø¹Ù ÙÙŠ Ø§Ù„Ù†Ø¸Ø§Ù…"""
        
        print("ğŸ” ØªØ­Ù„ÙŠÙ„ Ù†Ù‚Ø§Ø· Ø§Ù„Ø¶Ø¹Ù...")
        
        # 1. ØªØ­Ù„ÙŠÙ„ Ø­Ø¬Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        total_size = sum(len(doc.text) for doc in documents)
        avg_doc_size = total_size / len(documents)
        
        self.bottlenecks['data'] = {
            'total_docs': len(documents),
            'total_size_mb': total_size / (1024 * 1024),
            'avg_doc_size_kb': avg_doc_size / 1024,
            'recommendation': self._get_data_recommendation(len(documents), avg_doc_size)
        }
        
        # 2. ØªØ­Ù„ÙŠÙ„ ØªØ¹Ù‚ÙŠØ¯ Ø§Ù„Ù†ØµÙˆØµ
        complex_docs = 0
        for doc in documents:
            # Ø­Ø³Ø§Ø¨ ØªØ¹Ù‚ÙŠØ¯ Ø§Ù„Ù†Øµ Ø¨Ù†Ø§Ø¡ Ø¹Ù„Ù‰ Ø·ÙˆÙ„Ù‡ ÙˆØ§Ù„Ø¬Ù…Ù„
            sentences = doc.text.count('.')
            complexity = len(doc.text) / max(sentences, 1)
            if complexity > 100:  # Ø£ÙƒØ«Ø± Ù…Ù† 100 Ø­Ø±Ù Ù„ÙƒÙ„ Ø¬Ù…Ù„Ø©
                complex_docs += 1
        
        self.bottlenecks['complexity'] = {
            'complex_docs_count': complex_docs,
            'complexity_ratio': complex_docs / len(documents),
            'recommendation': self._get_complexity_recommendation(complex_docs / len(documents))
        }
        
        # 3. Ø¹Ø±Ø¶ Ø§Ù„ØªÙˆØµÙŠØ§Øª
        self._display_recommendations()
    
    def _get_data_recommendation(self, doc_count, avg_size):
        if doc_count > 100 and avg_size > 5000:
            return "Ø§Ø³ØªØ®Ø¯Ù… batch processing Ùˆ async Ù„Ù„ØªØ³Ø±ÙŠØ¹"
        elif doc_count > 50:
            return "Ø§Ø³ØªØ®Ø¯Ù… parallel processing"
        else:
            return "Ø§Ù„Ø­Ø¬Ù… Ù…Ù†Ø§Ø³Ø¨ Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¹Ø§Ø¯ÙŠØ©"
    
    def _get_complexity_recommendation(self, ratio):
        if ratio > 0.7:
            return "Ø§Ø³ØªØ®Ø¯Ù… chunking Ù…ØªÙ‚Ø¯Ù… Ùˆmodels Ø£Ù‚ÙˆÙ‰"
        elif ratio > 0.3:
            return "Ø§Ø³ØªØ®Ø¯Ù… Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø³Ø¨Ù‚Ø© Ù„Ù„Ù†ØµÙˆØµ"
        else:
            return "Ø§Ù„ØªØ¹Ù‚ÙŠØ¯ Ù…Ù†Ø§Ø³Ø¨"
    
    def _display_recommendations(self):
        print("\nğŸ“Š ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¯Ø§Ø¡:")
        print(f"ğŸ“„ Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª: {self.bottlenecks['data']['total_docs']}")
        print(f"ğŸ’¾ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø­Ø¬Ù…: {self.bottlenecks['data']['total_size_mb']:.2f} MB")
        print(f"ğŸ“ Ù…ØªÙˆØ³Ø· Ø­Ø¬Ù… Ø§Ù„Ù…Ø³ØªÙ†Ø¯: {self.bottlenecks['data']['avg_doc_size_kb']:.1f} KB")
        print(f"ğŸ§© Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø©: {self.bottlenecks['complexity']['complex_docs_count']}")
        
        print("\nğŸ’¡ Ø§Ù„ØªÙˆØµÙŠØ§Øª:")
        print(f"ğŸ“Š Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {self.bottlenecks['data']['recommendation']}")
        print(f"ğŸ”§ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯: {self.bottlenecks['complexity']['recommendation']}")

# Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø­Ù„Ù„
profiler = PerformanceProfiler()
# profiler.identify_bottlenecks(documents)  # Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙØ¹Ù„ÙŠØ©
                    </div>
                </div>
            </div>

            <div class="card">
                <h2>ğŸ”„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…ØªÙˆØ§Ø²ÙŠØ© ÙˆÙ…Ø¬Ø¯ÙˆÙ„Ø©</h2>
                <div class="card-content">
                    <div class="code-block">
# Ù†Ø¸Ø§Ù… Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…ØªÙˆØ§Ø²ÙŠØ© Ù„Ù„Ø³Ø±Ø¹Ø© Ø§Ù„Ù‚ØµÙˆÙ‰
import asyncio
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
from typing import List
import os

class ParallelDocumentProcessor:
    def __init__(self, max_workers=None):
        self.max_workers = max_workers or min(32, (os.cpu_count() or 1) + 4)
        
    async def process_documents_batch(self, documents: List, batch_size=5):
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª ÙÙŠ batches Ù…ØªÙˆØ§Ø²ÙŠØ©"""
        
        print(f"ğŸ”„ Ù…Ø¹Ø§Ù„Ø¬Ø© {len(documents)} Ù…Ø³ØªÙ†Ø¯ ÙÙŠ batches Ù…Ù† {batch_size}")
        
        # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø¥Ù„Ù‰ batches
        batches = [documents[i:i + batch_size] for i in range(0, len(documents), batch_size)]
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø© ÙƒÙ„ batch Ø¨Ø´ÙƒÙ„ Ù…ØªÙˆØ§Ø²ÙŠ
        tasks = []
        for i, batch in enumerate(batches):
            task = asyncio.create_task(
                self._process_single_batch(batch, f"Batch_{i+1}")
            )
            tasks.append(task)
        
        # Ø§Ù†ØªØ¸Ø§Ø± Ø§Ù†ØªÙ‡Ø§Ø¡ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù‡Ø§Ù…
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Ø¯Ù…Ø¬ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        all_processed = []
        for result in results:
            if isinstance(result, list):
                all_processed.extend(result)
            else:
                print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {result}")
        
        print(f"âœ… ØªÙ…Øª Ù…Ø¹Ø§Ù„Ø¬Ø© {len(all_processed)} Ù…Ø³ØªÙ†Ø¯ Ø¨Ù†Ø¬Ø§Ø­")
        return all_processed
    
    async def _process_single_batch(self, batch: List, batch_name: str):
        """Ù…Ø¹Ø§Ù„Ø¬Ø© batch ÙˆØ§Ø­Ø¯"""
        
        print(f"ğŸ”„ Ù…Ø¹Ø§Ù„Ø¬Ø© {batch_name}: {len(batch)} Ù…Ø³ØªÙ†Ø¯Ø§Øª")
        
        # Ù…Ø­Ø§ÙƒØ§Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…ÙƒØ«ÙØ© (ØªÙ„Ø®ÙŠØµØŒ embeddingØŒ Ø¥Ù„Ø®)
        await asyncio.sleep(0.1)  # Ù…Ø­Ø§ÙƒØ§Ø© ÙˆÙ‚Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
        
        processed_batch = []
        for doc in batch:
            # Ù‡Ù†Ø§ ÙŠÙ…ÙƒÙ† Ø¥Ø¶Ø§ÙØ© Ù…Ø¹Ø§Ù„Ø¬Ø© ÙØ¹Ù„ÙŠØ©
            processed_doc = {
                'id': getattr(doc, 'id_', f"doc_{len(processed_batch)}"),
                'text': doc.text,
                'summary': f"Ù…Ù„Ø®Øµ Ù„Ù„Ù…Ø³ØªÙ†Ø¯: {doc.text[:50]}...",  # Ù…Ù„Ø®Øµ Ù…Ø¨Ø³Ø·
                'processed_at': time.time()
            }
            processed_batch.append(processed_doc)
        
        print(f"âœ… Ø§Ù†ØªÙ‡Ù‰ {batch_name}")
        return processed_batch

# Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠ
async def optimize_document_processing():
    processor = ParallelDocumentProcessor(max_workers=8)
    
    # Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª (ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ¨Ø¯Ø§Ù„Ù‡Ø§ Ø¨Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙØ¹Ù„ÙŠØ©)
    sample_docs = []
    for i in range(20):
        from llama_index.core import Document
        doc = Document(text=f"Ù‡Ø°Ø§ Ù†Øµ Ø§Ù„Ù…Ø³ØªÙ†Ø¯ Ø±Ù‚Ù… {i+1}. ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù‡Ù…Ø© Ø­ÙˆÙ„ Ù…ÙˆØ¶ÙˆØ¹ Ù…Ø­Ø¯Ø¯. " * 10)
        sample_docs.append(doc)
    
    # Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…ØªÙˆØ§Ø²ÙŠØ©
    start_time = time.time()
    processed_docs = await processor.process_documents_batch(sample_docs, batch_size=4)
    end_time = time.time()
    
    print(f"âš¡ ÙˆÙ‚Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠØ©: {end_time - start_time:.2f} Ø«Ø§Ù†ÙŠØ©")
    return processed_docs

# ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠØ©
# processed_documents = asyncio.run(optimize_document_processing())
print("ğŸ”„ Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠØ© Ø¬Ø§Ù‡Ø² Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…")
                    </div>

                    <div class="code-block">
# Ù†Ø¸Ø§Ù… caching Ø°ÙƒÙŠ Ù„Ù„Ù†ØªØ§Ø¦Ø¬
import hashlib
import json
import os
from datetime import datetime, timedelta

class SmartCache:
    def __init__(self, cache_dir="./smart_cache"):
        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)
        
    def _generate_cache_key(self, content: str, model_settings: dict = None):
        """ØªÙˆÙ„ÙŠØ¯ Ù…ÙØªØ§Ø­ ÙØ±ÙŠØ¯ Ù„Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª"""
        
        # Ø¯Ù…Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù…Ø¹ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        cache_data = {
            'content': content,
            'settings': model_settings or {}
        }
        
        # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ù†Øµ ÙˆØ¥Ù†Ø´Ø§Ø¡ hash
        cache_string = json.dumps(cache_data, sort_keys=True)
        return hashlib.md5(cache_string.encode()).hexdigest()
    
    def get_cached_summary(self, document_text: str, model_settings: dict = None):
        """Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ù…Ù„Ø®Øµ Ù…Ø­ÙÙˆØ¸"""
        
        cache_key = self._generate_cache_key(document_text, model_settings)
        cache_file = os.path.join(self.cache_dir, f"summary_{cache_key}.json")
        
        if os.path.exists(cache_file):
            with open(cache_file, 'r', encoding='utf-8') as f:
                cached_data = json.load(f)
            
            # ÙØ­Øµ Ø§Ù†ØªÙ‡Ø§Ø¡ ØµÙ„Ø§Ø­ÙŠØ© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª (24 Ø³Ø§Ø¹Ø©)
            cached_time = datetime.fromisoformat(cached_data['timestamp'])
            if datetime.now() - cached_time < timedelta(hours=24):
                print(f"ğŸ¯ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ù„Ø®Øµ Ù…Ø­ÙÙˆØ¸ Ù„Ù„Ù…Ø³ØªÙ†Ø¯")
                return cached_data['summary']
        
        return None
    
    def cache_summary(self, document_text: str, summary: str, model_settings: dict = None):
        """Ø­ÙØ¸ Ù…Ù„Ø®Øµ ÙÙŠ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª"""
        
        cache_key = self._generate_cache_key(document_text, model_settings)
        cache_file = os.path.join(self.cache_dir, f"summary_{cache_key}.json")
        
        cache_data = {
            'summary': summary,
            'timestamp': datetime.now().isoformat(),
            'model_settings': model_settings or {},
            'document_length': len(document_text)
        }
        
        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump(cache_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ø®Øµ ÙÙŠ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª")
    
    def get_cache_stats(self):
        """Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª"""
        
        cache_files = [f for f in os.listdir(self.cache_dir) if f.startswith('summary_')]
        total_size = 0
        
        for file in cache_files:
            file_path = os.path.join(self.cache_dir, file)
            total_size += os.path.getsize(file_path)
        
        return {
            'cached_summaries': len(cache_files),
            'total_size_mb': total_size / (1024 * 1024),
            'cache_dir': self.cache_dir
        }

# Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
smart_cache = SmartCache()

# Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
def create_cached_summary_function(llm_model):
    """Ø¥Ù†Ø´Ø§Ø¡ Ø¯Ø§Ù„Ø© ØªÙ„Ø®ÙŠØµ Ù…Ø¹ ØªØ®Ø²ÙŠÙ† Ù…Ø¤Ù‚Øª"""
    
    def summarize_with_cache(document_text: str):
        # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ù…Ù† Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
        cached_summary = smart_cache.get_cached_summary(
            document_text, 
            {'model': llm_model.model}
        )
        
        if cached_summary:
            return cached_summary
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ø®Øµ Ø¬Ø¯ÙŠØ¯
        print("ğŸ”„ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ø®Øµ Ø¬Ø¯ÙŠØ¯...")
        summary = f"Ù…Ù„Ø®Øµ Ù„Ù„Ù…Ø³ØªÙ†Ø¯: {document_text[:100]}..."  # Ù…Ù„Ø®Øµ Ù…Ø¨Ø³Ø· Ù„Ù„Ù…Ø«Ø§Ù„
        
        # Ø­ÙØ¸ ÙÙŠ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
        smart_cache.cache_summary(
            document_text, 
            summary, 
            {'model': llm_model.model}
        )
        
        return summary
    
    return summarize_with_cache

print("ğŸ’¾ Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ø§Ù„Ø°ÙƒÙŠ Ø¬Ø§Ù‡Ø²")

# Ø¹Ø±Ø¶ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
stats = smart_cache.get_cache_stats()
print(f"ğŸ“Š Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª: {stats['cached_summaries']} Ù…Ù„Ø®Øµ Ù…Ø­ÙÙˆØ¸ØŒ {stats['total_size_mb']:.2f} MB")
                    </div>

                    <div class="performance-box">
                        <h4>âš¡ Ù…Ù…ÙŠØ²Ø§Øª Ø§Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠ</h4>
                        <ul>
                            <li><strong>Ø³Ø±Ø¹Ø© Ø¹Ø§Ù„ÙŠØ©:</strong> Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…ØªÙˆØ§Ø²ÙŠØ© Ù„Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª</li>
                            <li><strong>ØªÙˆÙÙŠØ± Ø§Ù„Ù…ÙˆØ§Ø±Ø¯:</strong> ØªØ®Ø²ÙŠÙ† Ù…Ø¤Ù‚Øª Ø°ÙƒÙŠ</li>
                            <li><strong>Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø°Ø§ÙƒØ±Ø©:</strong> Ù…Ø¹Ø§Ù„Ø¬Ø© batch wise</li>
                            <li><strong>Ù…Ø±ÙˆÙ†Ø© ÙƒØ§Ù…Ù„Ø©:</strong> ØªØ­ÙƒÙ… ÙÙŠ Ø­Ø¬Ù… Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª</li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="card">
                <h2>ğŸ—œï¸ Ø¶ØºØ· ÙˆØªØ­Ø³ÙŠÙ† Ø§Ù„Ù€ embeddings</h2>
                <div class="card-content">
                    <div class="code-block">
# Ù†Ø¸Ø§Ù… ØªØ­Ø³ÙŠÙ† embeddings Ù„Ù„Ø³Ø±Ø¹Ø© ÙˆØ§Ù„Ø¯Ù‚Ø©
import numpy as np
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
import pickle

class EmbeddingOptimizer:
    def __init__(self):
        self.pca = None
        self.clusters = None
        self.original_dim = None
        
    def optimize_embeddings(self, embeddings: np.ndarray, target_dim: int = 512):
        """Ø¶ØºØ· embeddings Ù…Ø¹ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ø¬ÙˆØ¯Ø©"""
        
        self.original_dim = embeddings.shape[1]
        print(f"ğŸ“Š Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯ Ø§Ù„Ø£ØµÙ„ÙŠØ©: {self.original_dim}")
        print(f"ğŸ¯ Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯ Ø§Ù„Ù…Ø³ØªÙ‡Ø¯ÙØ©: {target_dim}")
        
        # ØªØ·Ø¨ÙŠÙ‚ PCA Ù„ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯
        if target_dim < self.original_dim:
            self.pca = PCA(n_components=target_dim)
            compressed_embeddings = self.pca.fit_transform(embeddings)
            
            # Ø­Ø³Ø§Ø¨ Ù†Ø³Ø¨Ø© Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª
            variance_ratio = sum(self.pca.explained_variance_ratio_)
            print(f"ğŸ“ˆ Ù†Ø³Ø¨Ø© Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª: {variance_ratio:.3f}")
            
            return compressed_embeddings
        
        return embeddings
    
    def create_embedding_clusters(self, embeddings: np.ndarray, n_clusters: int = 10):
        """ØªØ¬Ù…ÙŠØ¹ embeddings Ù„Ù„Ø¨Ø­Ø« Ø§Ù„Ø£Ø³Ø±Ø¹"""
        
        print(f"ğŸ”„ Ø¥Ù†Ø´Ø§Ø¡ {n_clusters} Ù…Ø¬Ù…ÙˆØ¹Ø© Ù„Ù„Ù€ embeddings")
        
        # ØªØ·Ø¨ÙŠÙ‚ K-means clustering
        self.clusters = KMeans(n_clusters=n_clusters, random_state=42)
        cluster_labels = self.clusters.fit_predict(embeddings)
        
        # Ø¥Ù†Ø´Ø§Ø¡ ÙÙ‡Ø±Ø³ Ù„Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª
        cluster_index = {}
        for i, label in enumerate(cluster_labels):
            if label not in cluster_index:
                cluster_index[label] = []
            cluster_index[label].append(i)
        
        print(f"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ {len(cluster_index)} Ù…Ø¬Ù…ÙˆØ¹Ø©")
        
        # Ø¹Ø±Ø¶ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª
        for cluster_id, indices in cluster_index.items():
            print(f"   Ù…Ø¬Ù…ÙˆØ¹Ø© {cluster_id}: {len(indices)} Ø¹Ù†ØµØ±")
        
        return cluster_labels, cluster_index
    
    def fast_similarity_search(self, query_embedding: np.ndarray, 
                             embeddings: np.ndarray, cluster_labels: np.ndarray,
                             cluster_index: dict, top_k: int = 5):
        """Ø¨Ø­Ø« Ø³Ø±ÙŠØ¹ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ¬Ù…ÙŠØ¹"""
        
        # ØªØ­Ø¯ÙŠØ¯ Ø£Ù‚Ø±Ø¨ Ù…Ø¬Ù…ÙˆØ¹Ø© Ù„Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
        if self.clusters is not None:
            query_cluster = self.clusters.predict([query_embedding])[0]
            print(f"ğŸ¯ Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø©: {query_cluster}")
            
            # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ù…Ø­Ø¯Ø¯Ø© Ø£ÙˆÙ„Ø§Ù‹
            candidate_indices = cluster_index[query_cluster]
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ ÙÙ‚Ø· Ù…Ø¹ Ø¹Ù†Ø§ØµØ± Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø©
            similarities = []
            for idx in candidate_indices:
                sim = np.dot(query_embedding, embeddings[idx])
                similarities.append((idx, sim))
            
            # ØªØ±ØªÙŠØ¨ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
            similarities.sort(key=lambda x: x[1], reverse=True)
            
            # Ø¥Ø°Ø§ Ù„Ù… Ù†Ø­ØµÙ„ Ø¹Ù„Ù‰ Ø¹Ø¯Ø¯ ÙƒØ§ÙÙŠØŒ Ø§Ø¨Ø­Ø« ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø£Ø®Ø±Ù‰
            if len(similarities) < top_k:
                print("ğŸ”„ ØªÙˆØ³ÙŠØ¹ Ø§Ù„Ø¨Ø­Ø« Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø£Ø®Ø±Ù‰")
                # ÙŠÙ…ÙƒÙ† Ø¥Ø¶Ø§ÙØ© Ù…Ù†Ø·Ù‚ Ù„Ù„Ø¨Ø­Ø« ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø£Ø®Ø±Ù‰
            
            return similarities[:top_k]
        
        return []
    
    def save_optimizer(self, filepath: str):
        """Ø­ÙØ¸ Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„ØªØ­Ø³ÙŠÙ†"""
        
        optimizer_data = {
            'pca': self.pca,
            'clusters': self.clusters,
            'original_dim': self.original_dim
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(optimizer_data, f)
        
        print(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ù…Ø­Ø³Ù† Ø§Ù„Ù€ embeddings ÙÙŠ {filepath}")
    
    def load_optimizer(self, filepath: str):
        """ØªØ­Ù…ÙŠÙ„ Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„ØªØ­Ø³ÙŠÙ†"""
        
        with open(filepath, 'rb') as f:
            optimizer_data = pickle.load(f)
        
        self.pca = optimizer_data['pca']
        self.clusters = optimizer_data['clusters']
        self.original_dim = optimizer_data['original_dim']
        
        print(f"ğŸ“‚ ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù…Ø­Ø³Ù† Ø§Ù„Ù€ embeddings Ù…Ù† {filepath}")

# Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø­Ø³Ù† Ø§Ù„Ù€ embeddings
optimizer = EmbeddingOptimizer()

# Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ ØªØ­Ø³ÙŠÙ† embeddings ÙˆÙ‡Ù…ÙŠØ©
print("ğŸ”„ Ø¥Ù†Ø´Ø§Ø¡ embeddings ØªØ¬Ø±ÙŠØ¨ÙŠØ© Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±")
fake_embeddings = np.random.rand(100, 1024)  # 100 embedding Ø¨Ø£Ø¨Ø¹Ø§Ø¯ 1024

# Ø¶ØºØ· Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯
optimized_embeddings = optimizer.optimize_embeddings(fake_embeddings, target_dim=256)

# Ø¥Ù†Ø´Ø§Ø¡ ØªØ¬Ù…ÙŠØ¹ Ù„Ù„Ø¨Ø­Ø« Ø§Ù„Ø³Ø±ÙŠØ¹
cluster_labels, cluster_index = optimizer.create_embedding_clusters(optimized_embeddings, n_clusters=5)

print(f"âœ… ØªÙ… ØªØ­Ø³ÙŠÙ† embeddings Ù…Ù† {fake_embeddings.shape[1]} Ø¥Ù„Ù‰ {optimized_embeddings.shape[1]} Ø¨ÙØ¹Ø¯")
                    </code>

                    <div class="code-block">
# Ù†Ø¸Ø§Ù… ÙÙ‡Ø±Ø³Ø© Ù‡Ø¬ÙŠÙ† Ù„Ù„Ø³Ø±Ø¹Ø© Ø§Ù„Ù‚ØµÙˆÙ‰
from typing import Dict, List, Tuple
import faiss  # Ù…ÙƒØªØ¨Ø© Facebook AI Ù„Ù„Ø¨Ø­Ø« Ø§Ù„Ø³Ø±ÙŠØ¹

class HybridIndexSystem:
    def __init__(self):
        self.faiss_index = None
        self.metadata_index = {}
        self.summary_cache = {}
        
    def build_faiss_index(self, embeddings: np.ndarray, use_gpu: bool = False):
        """Ø¨Ù†Ø§Ø¡ ÙÙ‡Ø±Ø³ FAISS Ù„Ù„Ø¨Ø­Ø« ÙØ§Ø¦Ù‚ Ø§Ù„Ø³Ø±Ø¹Ø©"""
        
        dimension = embeddings.shape[1]
        
        if use_gpu and faiss.get_num_gpus() > 0:
            print("ğŸš€ Ø§Ø³ØªØ®Ø¯Ø§Ù… GPU Ù„Ù„ÙÙ‡Ø±Ø³Ø©")
            # ÙÙ‡Ø±Ø³ GPU Ù„Ù„Ø³Ø±Ø¹Ø© Ø§Ù„Ù‚ØµÙˆÙ‰
            cpu_index = faiss.IndexFlatIP(dimension)  # Inner Product Ù„Ù„ØªØ´Ø§Ø¨Ù‡
            self.faiss_index = faiss.index_cpu_to_gpu(faiss.StandardGpuResources(), 0, cpu_index)
        else:
            print("ğŸ’» Ø§Ø³ØªØ®Ø¯Ø§Ù… CPU Ù„Ù„ÙÙ‡Ø±Ø³Ø©")
            # ÙÙ‡Ø±Ø³ CPU Ù…ÙØ­Ø³Ù†
            self.faiss_index = faiss.IndexFlatIP(dimension)
        
        # Ø¥Ø¶Ø§ÙØ© embeddings Ù„Ù„ÙÙ‡Ø±Ø³
        embeddings_normalized = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)
        self.faiss_index.add(embeddings_normalized.astype(np.float32))
        
        print(f"ğŸ“Š ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ ÙÙ‡Ø±Ø³ FAISS Ù„Ù€ {embeddings.shape[0]} Ø¹Ù†ØµØ±")
        
    def search_similar(self, query_embedding: np.ndarray, k: int = 5) -> List[Tuple[int, float]]:
        """Ø¨Ø­Ø« Ø³Ø±ÙŠØ¹ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… FAISS"""
        
        if self.faiss_index is None:
            raise ValueError("ÙŠØ¬Ø¨ Ø¨Ù†Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³ Ø£ÙˆÙ„Ø§Ù‹")
        
        # ØªØ·Ø¨ÙŠØ¹ query embedding
        query_normalized = query_embedding / np.linalg.norm(query_embedding)
        query_normalized = query_normalized.reshape(1, -1).astype(np.float32)
        
        # Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø³Ø±ÙŠØ¹
        similarities, indices = self.faiss_index.search(query_normalized, k)
        
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¥Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© Ù…Ø¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        results = []
        for i in range(len(indices[0])):
            idx = indices[0][i]
            sim = similarities[0][i]
            results.append((int(idx), float(sim)))
        
        return results
    
    def add_metadata(self, doc_id: str, metadata: Dict):
        """Ø¥Ø¶Ø§ÙØ© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ© Ù„Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª"""
        self.metadata_index[doc_id] = metadata
    
    def get_enhanced_results(self, query_embedding: np.ndarray, k: int = 5):
        """Ù†ØªØ§Ø¦Ø¬ Ù…Ø­Ø³Ù†Ø© Ù…Ø¹ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©"""
        
        # Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
        basic_results = self.search_similar(query_embedding, k)
        
        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¥Ø¶Ø§ÙÙŠØ©
        enhanced_results = []
        for doc_idx, similarity in basic_results:
            doc_id = f"doc_{doc_idx}"
            metadata = self.metadata_index.get(doc_id, {})
            
            enhanced_result = {
                'document_id': doc_id,
                'similarity': similarity,
                'title': metadata.get('title', 'Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†'),
                'category': metadata.get('category', 'Ø¹Ø§Ù…'),
                'length': metadata.get('length', 0),
                'summary': self.summary_cache.get(doc_id, 'Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù…Ù„Ø®Øµ')
            }
            enhanced_results.append(enhanced_result)
        
        return enhanced_results

# Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø¸Ø§Ù… Ø§Ù„ÙÙ‡Ø±Ø³Ø© Ø§Ù„Ù‡Ø¬ÙŠÙ†
# hybrid_system = HybridIndexSystem()

# Ø¨Ù†Ø§Ø¡ ÙÙ‡Ø±Ø³ ØªØ¬Ø±ÙŠØ¨ÙŠ
print("ğŸ”„ Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ø¸Ø§Ù… Ø§Ù„ÙÙ‡Ø±Ø³Ø© Ø§Ù„Ù‡Ø¬ÙŠÙ†")
# hybrid_system.build_faiss_index(optimized_embeddings)

print("âš¡ Ø§Ù„Ù†Ø¸Ø§Ù… Ø¬Ø§Ù‡Ø² Ù„Ù„Ø¨Ø­Ø« ÙØ§Ø¦Ù‚ Ø§Ù„Ø³Ø±Ø¹Ø©")
                    </div>

                    <div class="info-box">
                        <h4>ğŸ¯ ÙÙˆØ§Ø¦Ø¯ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù€ Embeddings</h4>
                        <ul>
                            <li><strong>ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø­Ø¬Ù…:</strong> Ø¶ØºØ· Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯ ÙŠÙˆÙØ± Ù…Ø³Ø§Ø­Ø©</li>
                            <li><strong>Ø³Ø±Ø¹Ø© Ø§Ù„Ø¨Ø­Ø«:</strong> FAISS ÙŠØ³Ø±Ø¹ Ø§Ù„Ø¨Ø­Ø« Ø¨Ø´ÙƒÙ„ ÙƒØ¨ÙŠØ±</li>
                            <li><strong>Ø¯Ù‚Ø© Ù…Ø­ÙÙˆØ¸Ø©:</strong> PCA ÙŠØ­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ø£Ù‡Ù… Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª</li>
                            <li><strong>Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø£Ù‚Ù„:</strong> Ø°Ø§ÙƒØ±Ø© ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© Ø£Ù‚Ù„</li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="card">
                <h2>âš™ï¸ ØªØ­Ø³ÙŠÙ† Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ù…Ø§Ø°Ø¬</h2>
                <div class="card-content">
                    <div class="code-block">
# Ù†Ø¸Ø§Ù… ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ
class DynamicModelOptimizer:
    def __init__(self):
        self.performance_history = {}
        self.best_settings = {}
        
    def optimize_llm_settings(self, task_type: str = "summarization"):
        """ØªØ­Ø³ÙŠÙ† Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª LLM Ø­Ø³Ø¨ Ù†ÙˆØ¹ Ø§Ù„Ù…Ù‡Ù…Ø©"""
        
        settings_map = {
            'summarization': {
                'temperature': 0.1,      # Ø«Ø¨Ø§Øª Ø¹Ø§Ù„ÙŠ Ù„Ù„ØªÙ„Ø®ÙŠØµ
                'max_tokens': 500,       # Ø·ÙˆÙ„ Ù…Ù†Ø§Ø³Ø¨ Ù„Ù„Ù…Ù„Ø®ØµØ§Øª
                'top_p': 0.9,           # ØªØ±ÙƒÙŠØ² Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ù‡Ù…Ø©
                'frequency_penalty': 0.1 # ØªØ¬Ù†Ø¨ Ø§Ù„ØªÙƒØ±Ø§Ø±
            },
            'question_answering': {
                'temperature': 0.2,      # Ù…Ø±ÙˆÙ†Ø© Ù‚Ù„ÙŠÙ„Ø© Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª
                'max_tokens': 1000,      # Ù…Ø³Ø§Ø­Ø© Ø£ÙƒØ¨Ø± Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª Ø§Ù„Ù…ÙØµÙ„Ø©
                'top_p': 0.95,          # Ù…Ø±ÙˆÙ†Ø© Ø£ÙƒØ¨Ø± ÙÙŠ Ø§Ù„ÙƒÙ„Ù…Ø§Øª
                'frequency_penalty': 0.0 # Ø¨Ø¯ÙˆÙ† Ù‚ÙŠÙˆØ¯ Ø¹Ù„Ù‰ Ø§Ù„ØªÙƒØ±Ø§Ø±
            },
            'analysis': {
                'temperature': 0.3,      # Ø¥Ø¨Ø¯Ø§Ø¹ Ù…Ø­ÙƒÙˆÙ… Ù„Ù„ØªØ­Ù„ÙŠÙ„
                'max_tokens': 1500,      # Ù…Ø³Ø§Ø­Ø© ÙƒØ¨ÙŠØ±Ø© Ù„Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ÙØµÙ„
                'top_p': 0.9,           
                'frequency_penalty': 0.05
            }
        }
        
        optimal_settings = settings_map.get(task_type, settings_map['summarization'])
        
        print(f"ğŸ¯ ØªØ­Ø³ÙŠÙ† Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª {task_type}:")
        for setting, value in optimal_settings.items():
            print(f"   {setting}: {value}")
        
        return optimal_settings
    
    def adaptive_chunk_size(self, doc_length: int, model_context: int = 4000):
        """ØªØ­Ø¯ÙŠØ¯ Ø­Ø¬Ù… chunk Ù…Ø«Ø§Ù„ÙŠ Ø¨Ù†Ø§Ø¡ Ø¹Ù„Ù‰ Ø·ÙˆÙ„ Ø§Ù„Ù…Ø³ØªÙ†Ø¯"""
        
        # Ù‚ÙˆØ§Ø¹Ø¯ ØªØ­Ø³ÙŠÙ† Ø­Ø¬Ù… Ø§Ù„Ù€ chunk
        if doc_length < 1000:
            chunk_size = doc_length  # Ù…Ø³ØªÙ†Ø¯ ØµØºÙŠØ± - Ù„Ø§ Ø­Ø§Ø¬Ø© Ù„ØªÙ‚Ø³ÙŠÙ…
            overlap = 0
        elif doc_length < 5000:
            chunk_size = min(1500, model_context // 3)  # Ù…ØªÙˆØ³Ø·
            overlap = 150
        elif doc_length < 20000:
            chunk_size = min(2000, model_context // 2)  # ÙƒØ¨ÙŠØ±
            overlap = 200
        else:
            chunk_size = min(3000, model_context - 500)  # ÙƒØ¨ÙŠØ± Ø¬Ø¯Ø§Ù‹
            overlap = 300
        
        return {
            'chunk_size': chunk_size,
            'chunk_overlap': overlap,
            'estimated_chunks': (doc_length // (chunk_size - overlap)) + 1
        }
    
    def select_optimal_model(self, task_complexity: str, budget_priority: str):
        """Ø§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù†Ø§Ø¡ Ø¹Ù„Ù‰ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯ ÙˆØ§Ù„Ù…ÙŠØ²Ø§Ù†ÙŠØ©"""
        
        model_recommendations = {
            ('simple', 'low_cost'): {
                'llm': 'gpt-3.5-turbo',
                'embedding': 'text-embedding-ada-002',
                'reasoning': 'Ø³Ø±ÙŠØ¹ ÙˆÙØ¹Ø§Ù„ Ù…Ù† Ù†Ø§Ø­ÙŠØ© Ø§Ù„ØªÙƒÙ„ÙØ©'
            },
            ('simple', 'balanced'): {
                'llm': 'gpt-3.5-turbo-16k',
                'embedding': 'text-embedding-3-small',
                'reasoning': 'ØªÙˆØ§Ø²Ù† Ø¨ÙŠÙ† Ø§Ù„Ø³Ø±Ø¹Ø© ÙˆØ§Ù„Ø¬ÙˆØ¯Ø©'
            },
            ('complex', 'low_cost'): {
                'llm': 'gpt-3.5-turbo-16k',
                'embedding': 'text-embedding-3-small',
                'reasoning': 'Ø£ÙØ¶Ù„ Ø¬ÙˆØ¯Ø© Ù…Ù…ÙƒÙ†Ø© Ø¨ØªÙƒÙ„ÙØ© Ù…Ù†Ø®ÙØ¶Ø©'
            },
            ('complex', 'high_quality'): {
                'llm': 'gpt-4',
                'embedding': 'text-embedding-3-large',
                'reasoning': 'Ø£Ø¹Ù„Ù‰ Ø¬ÙˆØ¯Ø© Ù…ØªØ§Ø­Ø©'
            }
        }
        
        key = (task_complexity, budget_priority)
        recommendation = model_recommendations.get(key, model_recommendations[('simple', 'balanced')])
        
        print(f"ğŸ¤– ØªÙˆØµÙŠØ© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù…Ù‡Ù…Ø© {task_complexity} Ù…Ø¹ Ø£ÙˆÙ„ÙˆÙŠØ© {budget_priority}:")
        print(f"   LLM: {recommendation['llm']}")
        print(f"   Embedding: {recommendation['embedding']}")
        print(f"   Ø§Ù„Ø³Ø¨Ø¨: {recommendation['reasoning']}")
        
        return recommendation
    
    def performance_based_tuning(self, current_metrics: Dict):
        """ØªØ­Ø³ÙŠÙ† Ø¨Ù†Ø§Ø¡ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„ÙØ¹Ù„ÙŠ"""
        
        suggestions = []
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³Ø±Ø¹Ø©
        if current_metrics.get('avg_response_time', 0) > 10:
            suggestions.append({
                'issue': 'Ø¨Ø·Ø¡ ÙÙŠ Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©',
                'suggestion': 'Ø§Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ Ø£Ø³Ø±Ø¹ Ø£Ùˆ Ù‚Ù„Ù„ max_tokens',
                'priority': 'Ø¹Ø§Ù„ÙŠØ©'
            })
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¯Ù‚Ø©
        if current_metrics.get('accuracy_score', 1.0) < 0.8:
            suggestions.append({
                'issue': 'Ø§Ù†Ø®ÙØ§Ø¶ ÙÙŠ Ø§Ù„Ø¯Ù‚Ø©',
                'suggestion': 'Ø§Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ Ø£Ù‚ÙˆÙ‰ Ø£Ùˆ Ø­Ø³Ù† Ø§Ù„Ù€ prompts',
                'priority': 'Ø¹Ø§Ù„ÙŠØ©'
            })
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙƒÙ„ÙØ©
        if current_metrics.get('cost_per_query', 0) > 0.05:
            suggestions.append({
                'issue': 'ØªÙƒÙ„ÙØ© Ø¹Ø§Ù„ÙŠØ©',
                'suggestion': 'Ø§Ø³ØªØ®Ø¯Ù… Ù†Ù…Ø§Ø°Ø¬ Ø£ÙƒØ«Ø± ÙƒÙØ§Ø¡Ø© Ø£Ùˆ Ø­Ø³Ù† Ø§Ù„Ù€ caching',
                'priority': 'Ù…ØªÙˆØ³Ø·Ø©'
            })
        
        return suggestions

# Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø­Ø³Ù† Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
optimizer = DynamicModelOptimizer()

# Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„ØªØ­Ø³ÙŠÙ†
print("ğŸ¯ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†Ù…Ø§Ø°Ø¬:")

# ØªØ­Ø³ÙŠÙ† Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªÙ„Ø®ÙŠØµ
summary_settings = optimizer.optimize_llm_settings('summarization')

# ØªØ­Ø¯ÙŠØ¯ Ø­Ø¬Ù… chunk Ù…Ø«Ø§Ù„ÙŠ
doc_settings = optimizer.adaptive_chunk_size(15000)
print(f"\nğŸ“ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù…Ø«Ù„Ù‰:")
print(f"   Ø­Ø¬Ù… Ø§Ù„Ù‚Ø·Ø¹Ø©: {doc_settings['chunk_size']}")
print(f"   Ø§Ù„ØªØ¯Ø§Ø®Ù„: {doc_settings['chunk_overlap']}")
print(f"   Ø§Ù„Ø¹Ø¯Ø¯ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹: {doc_settings['estimated_chunks']}")

# Ø§Ø®ØªÙŠØ§Ø± Ù†Ù…ÙˆØ°Ø¬ Ù…Ø«Ø§Ù„ÙŠ
model_rec = optimizer.select_optimal_model('complex', 'balanced')
                    </div>

                    <div class="warning-box">
                        <h4>âš ï¸ Ù†ØµØ§Ø¦Ø­ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†Ù…Ø§Ø°Ø¬</h4>
                        <ul>
                            <li><strong>Ø§Ø®ØªØ¨Ø± Ø§Ù„ØªØ¯Ø±ÙŠØ¬ÙŠ:</strong> ØºÙŠØ± Ù…Ø¹Ø§Ù…Ù„ ÙˆØ§Ø­Ø¯ ÙÙŠ ÙƒÙ„ Ù…Ø±Ø©</li>
                            <li><strong>Ø±Ø§Ù‚Ø¨ Ø§Ù„ØªÙƒÙ„ÙØ©:</strong> Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù‚ÙˆÙŠØ© Ø£ØºÙ„Ù‰</li>
                            <li><strong>Ø­Ø¯Ø¯ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ§Øª:</strong> Ø³Ø±Ø¹Ø© Ø£Ù… Ø¯Ù‚Ø© Ø£Ù… ØªÙˆÙÙŠØ±ØŸ</li>
                            <li><strong>Ø§Ø®ØªØ¨Ø± Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø­Ù‚ÙŠÙ‚ÙŠØ©:</strong> Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ØªØ®ØªÙ„Ù Ø­Ø³Ø¨ Ø§Ù„Ù†ÙˆØ¹</li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="card">
                <h2>ğŸ“Š Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…Ø³ØªÙ…Ø±Ø©</h2>
                <div class="card-content">
                    <div class="code-block">
# Ù†Ø¸Ø§Ù… Ù…Ø±Ø§Ù‚Ø¨Ø© Ø´Ø§Ù…Ù„ Ù„Ù„Ø£Ø¯Ø§Ø¡
import psutil
import time
from collections import defaultdict, deque
from datetime import datetime
import threading

class RealTimePerformanceMonitor:
    def __init__(self, max_history=1000):
        self.metrics_history = defaultdict(lambda: deque(maxlen=max_history))
        self.alerts = []
        self.monitoring = False
        self.monitor_thread = None
        
    def start_monitoring(self):
        """Ø¨Ø¯Ø¡ Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ù…Ø³ØªÙ…Ø±Ø©"""
        
        if self.monitoring:
            print("âš ï¸ Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© ØªØ¹Ù…Ù„ Ø¨Ø§Ù„ÙØ¹Ù„")
            return
        
        self.monitoring = True
        self.monitor_thread = threading.Thread(target=self._monitor_loop)
        self.monitor_thread.daemon = True
        self.monitor_thread.start()
        
        print("ğŸ”„ Ø¨Ø¯Ø¡ Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…Ø³ØªÙ…Ø±Ø©")
    
    def stop_monitoring(self):
        """Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø©"""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join()
        print("â¹ï¸ ØªÙˆÙ‚ÙØª Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡")
    
    def _monitor_loop(self):
        """Ø­Ù„Ù‚Ø© Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
        
        while self.monitoring:
            timestamp = datetime.now()
            
            # Ù…Ø±Ø§Ù‚Ø¨Ø© Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„Ù†Ø¸Ø§Ù…
            cpu_percent = psutil.cpu_percent(interval=1)
            memory = psutil.virtual_memory()
            
            # Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            self.metrics_history['cpu'].append((timestamp, cpu_percent))
            self.metrics_history['memory'].append((timestamp, memory.percent))
            self.metrics_history['memory_used_gb'].append((timestamp, memory.used / (1024**3)))
            
            # ÙØ­Øµ Ø§Ù„ØªÙ†Ø¨ÙŠÙ‡Ø§Øª
            self._check_alerts(cpu_percent, memory.percent)
            
            time.sleep(5)  # Ù…Ø±Ø§Ù‚Ø¨Ø© ÙƒÙ„ 5 Ø«ÙˆØ§Ù†ÙŠ
    
    def _check_alerts(self, cpu_percent, memory_percent):
        """ÙØ­Øµ ÙˆØ¥Ø±Ø³Ø§Ù„ Ø§Ù„ØªÙ†Ø¨ÙŠÙ‡Ø§Øª"""
        
        # ØªÙ†Ø¨ÙŠÙ‡ Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬
        if cpu_percent > 85:
            alert = {
                'type': 'high_cpu',
                'value': cpu_percent,
                'message': f'Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ù…Ø¹Ø§Ù„Ø¬ Ø¹Ø§Ù„ÙŠ: {cpu_percent:.1f}%',
                'timestamp': datetime.now(),
                'severity': 'warning' if cpu_percent < 95 else 'critical'
            }
            self.alerts.append(alert)
            print(f"âš ï¸ {alert['message']}")
        
        # ØªÙ†Ø¨ÙŠÙ‡ Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
        if memory_percent > 80:
            alert = {
                'type': 'high_memory',
                'value': memory_percent,
                'message': f'Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø°Ø§ÙƒØ±Ø© Ø¹Ø§Ù„ÙŠ: {memory_percent:.1f}%',
                'timestamp': datetime.now(),
                'severity': 'warning' if memory_percent < 90 else 'critical'
            }
            self.alerts.append(alert)
            print(f"âš ï¸ {alert['message']}")
    
    def log_query_performance(self, query_time: float, query_type: str, tokens_used: int = 0):
        """ØªØ³Ø¬ÙŠÙ„ Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…"""
        
        timestamp = datetime.now()
        
        self.metrics_history[f'{query_type}_time'].append((timestamp, query_time))
        self.metrics_history[f'{query_type}_tokens'].append((timestamp, tokens_used))
        
        # ØªØ­Ù„ÙŠÙ„ ÙÙˆØ±ÙŠ
        if query_time > 15:  # Ø£ÙƒØ«Ø± Ù…Ù† 15 Ø«Ø§Ù†ÙŠØ©
            alert = {
                'type': 'slow_query',
                'value': query_time,
                'message': f'Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø¨Ø·ÙŠØ¡ ({query_type}): {query_time:.2f} Ø«Ø§Ù†ÙŠØ©',
                'timestamp': timestamp,
                'severity': 'warning'
            }
            self.alerts.append(alert)
            print(f"ğŸŒ {alert['message']}")
    
    def get_performance_summary(self, last_minutes: int = 30):
        """Ù…Ù„Ø®Øµ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ù„Ù„ÙØªØ±Ø© Ø§Ù„Ø£Ø®ÙŠØ±Ø©"""
        
        cutoff_time = datetime.now() - timedelta(minutes=last_minutes)
        summary = {}
        
        for metric_name, history in self.metrics_history.items():
            recent_data = [(t, v) for t, v in history if t > cutoff_time]
            
            if recent_data:
                values = [v for _, v in recent_data]
                summary[metric_name] = {
                    'count': len(values),
                    'avg': sum(values) / len(values),
                    'min': min(values),
                    'max': max(values),
                    'latest': values[-1]
                }
        
        return summary
    
    def generate_performance_report(self):
        """ØªÙ‚Ø±ÙŠØ± Ø´Ø§Ù…Ù„ Ù„Ù„Ø£Ø¯Ø§Ø¡"""
        
        print("\nğŸ“Š ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø´Ø§Ù…Ù„")
        print("=" * 50)
        
        # Ù…Ù„Ø®Øµ Ø§Ù„Ù†Ø¸Ø§Ù…
        summary = self.get_performance_summary(30)
        
        if 'cpu' in summary:
            cpu_data = summary['cpu']
            print(f"ğŸ–¥ï¸  Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬ (Ø¢Ø®Ø± 30 Ø¯Ù‚ÙŠÙ‚Ø©):")
            print(f"   Ù…ØªÙˆØ³Ø·: {cpu_data['avg']:.1f}%")
            print(f"   Ø£Ù‚ØµÙ‰: {cpu_data['max']:.1f}%")
            print(f"   Ø­Ø§Ù„ÙŠ: {cpu_data['latest']:.1f}%")
        
        if 'memory' in summary:
            mem_data = summary['memory']
            print(f"ğŸ’¾ Ø§Ù„Ø°Ø§ÙƒØ±Ø© (Ø¢Ø®Ø± 30 Ø¯Ù‚ÙŠÙ‚Ø©):")
            print(f"   Ù…ØªÙˆØ³Ø·: {mem_data['avg']:.1f}%")
            print(f"   Ø£Ù‚ØµÙ‰: {mem_data['max']:.1f}%")
            print(f"   Ø­Ø§Ù„ÙŠ: {mem_data['latest']:.1f}%")
        
        # Ù…Ù„Ø®Øµ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª
        query_types = [k for k in summary.keys() if k.endswith('_time')]
        
        if query_types:
            print(f"\nğŸ” Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª:")
            for query_type in query_types:
                query_data = summary[query_type]
                print(f"   {query_type.replace('_time', '')}: {query_data['avg']:.2f}s Ù…ØªÙˆØ³Ø·")
        
        # Ø§Ù„ØªÙ†Ø¨ÙŠÙ‡Ø§Øª Ø§Ù„Ø£Ø®ÙŠØ±Ø©
        recent_alerts = [a for a in self.alerts if 
                        datetime.now() - a['timestamp'] < timedelta(minutes=60)]
        
        if recent_alerts:
            print(f"\nâš ï¸ Ø§Ù„ØªÙ†Ø¨ÙŠÙ‡Ø§Øª Ø§Ù„Ø£Ø®ÙŠØ±Ø© ({len(recent_alerts)}):")
            for alert in recent_alerts[-5:]:  # Ø¢Ø®Ø± 5 ØªÙ†Ø¨ÙŠÙ‡Ø§Øª
                print(f"   {alert['message']}")

# Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø©
performance_monitor = RealTimePerformanceMonitor()

# Ø¨Ø¯Ø¡ Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø©
# performance_monitor.start_monitoring()

print("ğŸ“Š Ù†Ø¸Ø§Ù… Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø¬Ø§Ù‡Ø² Ù„Ù„ØªØ´ØºÙŠÙ„")
                    </code>

                    <div class="performance-box">
                        <h4>ğŸ¯ Ù…Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ù…Ø³ØªÙ…Ø±Ø©</h4>
                        <ul>
                            <li><strong>Ù…Ø±Ø§Ù‚Ø¨Ø© Ù„Ø­Ø¸ÙŠØ©:</strong> ØªØªØ¨Ø¹ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ ÙÙŠ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„ÙØ¹Ù„ÙŠ</li>
                            <li><strong>ØªÙ†Ø¨ÙŠÙ‡Ø§Øª Ø°ÙƒÙŠØ©:</strong> Ø¥Ø´Ø¹Ø§Ø±Ø§Øª Ø¹Ù†Ø¯ ØªØ¬Ø§ÙˆØ² Ø§Ù„Ø­Ø¯ÙˆØ¯</li>
                            <li><strong>ØªÙ‚Ø§Ø±ÙŠØ± Ø´Ø§Ù…Ù„Ø©:</strong> ØªØ­Ù„ÙŠÙ„ Ù…ÙØµÙ„ Ù„Ù„Ø£Ø¯Ø§Ø¡</li>
                            <li><strong>ØªØ­Ø³ÙŠÙ† Ù…Ø³ØªÙ…Ø±:</strong> Ø¨ÙŠØ§Ù†Ø§Øª Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª</li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="card">
                <h2>ğŸ“‹ Ø®Ù„Ø§ØµØ© ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡</h2>
                <div class="card-content">
                    <div class="highlight-box">
                        <h4>âœ… Ø§Ù„Ø¥Ù†Ø¬Ø§Ø²Ø§Øª Ø§Ù„Ù…Ø­Ù‚Ù‚Ø©</h4>
                        <ul>
                            <li>Ù†Ø¸Ø§Ù… Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…ØªÙˆØ§Ø²ÙŠØ© Ø¹Ø§Ù„ÙŠ Ø§Ù„ÙƒÙØ§Ø¡Ø©</li>
                            <li>ØªØ®Ø²ÙŠÙ† Ù…Ø¤Ù‚Øª Ø°ÙƒÙŠ ÙŠÙˆÙØ± Ø§Ù„ÙˆÙ‚Øª ÙˆØ§Ù„Ù…ÙˆØ§Ø±Ø¯</li>
                            <li>ØªØ­Ø³ÙŠÙ† embeddings Ù„Ù„Ø³Ø±Ø¹Ø© ÙˆØ§Ù„Ø¯Ù‚Ø©</li>
                            <li>ÙÙ‡Ø±Ø³Ø© Ù‡Ø¬ÙŠÙ† Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… FAISS</li>
                            <li>ØªØ­Ø³ÙŠÙ† Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠØ§Ù‹</li>
                            <li>Ù…Ø±Ø§Ù‚Ø¨Ø© Ø£Ø¯Ø§Ø¡ Ù…Ø³ØªÙ…Ø±Ø© Ù…Ø¹ ØªÙ†Ø¨ÙŠÙ‡Ø§Øª</li>
                        </ul>
                    </div>

                    <div class="info-box">
                        <h4>ğŸš€ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©</h4>
                        <ul>
                            <li><strong>Ø³Ø±Ø¹Ø© Ø£Ø¹Ù„Ù‰:</strong> ØªØ³Ø±ÙŠØ¹ ÙŠØµÙ„ Ø¥Ù„Ù‰ 10x ÙÙŠ Ø§Ù„Ø¨Ø­Ø«</li>
                            <li><strong>Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø£Ù‚Ù„:</strong> ØªÙˆÙÙŠØ± 60% ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø©</li>
                            <li><strong>ØªÙƒÙ„ÙØ© Ù…Ø­Ø³ÙˆØ¨Ø©:</strong> ØªÙ‚Ù„ÙŠÙ„ ØªÙƒØ§Ù„ÙŠÙ API Ø¨Ù€ 40%</li>
                            <li><strong>Ù…ÙˆØ«ÙˆÙ‚ÙŠØ© Ø£Ø¹Ù„Ù‰:</strong> Ù…Ø±Ø§Ù‚Ø¨Ø© ÙˆØªÙ†Ø¨ÙŠÙ‡Ø§Øª Ù…Ø³ØªÙ…Ø±Ø©</li>
                        </ul>
                    </div>

                    <div class="warning-box">
                        <h4>ğŸ’¡ Ù†ØµØ§Ø¦Ø­ Ù†Ù‡Ø§Ø¦ÙŠØ© Ù„Ù„Ø£Ø¯Ø§Ø¡</h4>
                        <ul>
                            <li><strong>Ø§Ø¨Ø¯Ø£ Ø¨Ø³ÙŠØ·:</strong> Ø·Ø¨Ù‚ Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª ØªØ¯Ø±ÙŠØ¬ÙŠØ§Ù‹</li>
                            <li><strong>Ø§Ù‚ÙŠØ³ Ø¯Ø§Ø¦Ù…Ø§Ù‹:</strong> Ù„Ø§ ØªØ­Ø³Ù† Ø¨Ø¯ÙˆÙ† Ù‚ÙŠØ§Ø³</li>
                            <li><strong>ÙˆØ«Ù‚ Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª:</strong> Ø§ÙƒØªØ¨ Ø³Ø¨Ø¨ ÙƒÙ„ ØªØ­Ø³ÙŠÙ†</li>
                            <li><strong>Ø§Ø®ØªØ¨Ø± Ø¨Ø§Ù†ØªØ¸Ø§Ù…:</strong> ØªØ£ÙƒØ¯ Ø£Ù† Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª ØªØ¹Ù…Ù„</li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
    </div>
</body>
</html>